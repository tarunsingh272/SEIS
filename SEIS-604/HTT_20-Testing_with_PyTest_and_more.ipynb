{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37d3ab2",
   "metadata": {},
   "source": [
    "## HTT20 - Testing with PyTest and More\n",
    "\n",
    "This notebook references these specific sections:\n",
    "\n",
    "Ch 20 \"Unit Testing\" in How to Think Like a Computer Scientist with Python (HTT):\n",
    "\n",
    "https://runestone.academy/ns/books/published/thinkcspy/UnitTesting/toctree.html?mode=browsing\n",
    "\n",
    "This notebook by:\n",
    "\n",
    "***Eric V. Level***  \n",
    "\n",
    "Graduate Programs in Software Engineering and Data Science  \n",
    "University of St Thomas\n",
    "St Paul, MN\n",
    "\n",
    "Includes material our primary online site...:\n",
    "\n",
    "- ***Problem Solving with Algorithms and Data Structures using Python***   \n",
    "by Brad Miller and David Ranum  \n",
    "Luther College \n",
    "(DSP for short)\n",
    "\n",
    "https://runestone.academy/ns/books/published/pythonds3/index.html?mode=browsing\n",
    "\n",
    "...along with material from this secondary source.\n",
    "\n",
    "- ***How to Think Like a Computer Scientist in Python\"***   \n",
    "by Brad Miller and David Ranum  \n",
    "Luther College \n",
    "(HTT for short)\n",
    "\n",
    "https://runestone.academy/ns/books/published/thinkcspy/index.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70627854",
   "metadata": {},
   "source": [
    "## HTT-20.1 - Introduction: Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705318c5",
   "metadata": {},
   "source": [
    "Testing plays an important role in the development of software. To this point, most of the testing you have done has probably involved running your program and fixing errors as you notice them. In this chapter, you will learn about a more methodical approach to testing. Along the way, you will pick up some design techniques. So, let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1520c7",
   "metadata": {},
   "source": [
    "## HTT-20.2 - Checking Assumptions With `assert`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c9203",
   "metadata": {},
   "source": [
    "Many functions work correctly only for certain parameter values, and produce invalid results (or crash) if given others. Consider the following function, which computes the sum of the numbers in a range specified by its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9baf4691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# htt20_2_1-ac_sumnums_1.py\n",
    "\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"returns the sum of the numbers in the range [lo..hi]\"\"\"\n",
    "\n",
    "    sum = 0\n",
    "    for i in range(lo, hi+1):\n",
    "        sum += i\n",
    "    return sum\n",
    "\n",
    "print(sumnums(1, 3))\n",
    "print(sumnums(3, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c6826",
   "metadata": {},
   "source": [
    "Notice that the first call to sumnums produces the correct answer (6), while the second call produces an incorrect answer. `sumnums` works correctly only if `lo` has a value that is less than, or equal to, `hi`.\n",
    "\n",
    "This function trusts the calling code to provide parameter values that are valid. If the caller provides a second parameter that is lower than the first parameter, the function does not produce a correct result. That’s not the fault of the function; the function isn’t designed to work correctly if `lo > hi`.\n",
    "\n",
    "To make it clear that the function is designed to work correctly only if `lo <= hi`, it’s a good idea to state that as a precondition in the function documentation, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473346b",
   "metadata": {},
   "source": [
    "```\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"returns the sum of the numbers in the range [lo..hi]\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550bd0a",
   "metadata": {},
   "source": [
    "***Precondition***\n",
    "\n",
    "A **precondition** specifies a condition that must be `True` if the function is to produce correct results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9778ee",
   "metadata": {},
   "source": [
    "A precondition places a constraint on the values of the parameters that the caller can pass and expect to receive a valid result. Preconditions are boolean expressions – comparisons that you might write in an `if` statement. We’ll have more to say about preconditions later in the chapter.\n",
    "\n",
    "Code that calls a function is responsible for passing parameters that satisfy the function’s preconditions. If the calling code passes values that violate the function’s preconditions, the function isn’t expected to work correctly. That’s not the function’s fault: it’s the caller’s fault for passing parameters to the function that the function is not designed to handle correctly. However, it might be a good idea if we designed the function to check for invalid values, and when it detects them, somehow report that it was called incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bba955",
   "metadata": {},
   "source": [
    "### HTT-20.2.1 - Designing Defensive Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d068f",
   "metadata": {},
   "source": [
    "A defensive function is a function that checks its parameters to see if they are valid, and responds in an appropriate way if they are invalid. That raises the question: what should a defensive function do if it receives invalid values? Should it print an error? Silently ignore the problem and return a default value? Return a special value that indicates an error? Exit the program? There are several options.\n",
    "\n",
    "As an example, here is one way we could make `sumnums` defensive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eeeaf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Alert: Invalid parameters to sumnums.\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# htt20_2_2-ac_sumnums_2.py\n",
    "\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"returns the sum of the numbers in the range [lo..hi]\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    \"\"\"\n",
    "\n",
    "    if lo > hi:\n",
    "        print('Alert: Invalid parameters to sumnums.')\n",
    "        return -1\n",
    "\n",
    "    sum = 0\n",
    "    for i in range(lo, hi+1):\n",
    "        sum += i\n",
    "    return sum\n",
    "\n",
    "print(sumnums(1, 3))\n",
    "print(sumnums(3, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d41a5",
   "metadata": {},
   "source": [
    "In this version, the function checks to see if the preconditions are violated, and if so, it complains by printing a message and returns the value -1 to the caller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b49270",
   "metadata": {},
   "source": [
    "***Defensive Programming***\n",
    "\n",
    "The strategy of designing functions that check their parameters embodies a principle of software design called ***defensive programming***, in which software checks for invalid inputs and responds in an appropriate way. Defensive programming is especially important for mission critical systems, but it can be a helpful strategy in regular software projects, as we’ll soon see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef69483",
   "metadata": {},
   "source": [
    "This is an improvement over the original function, because now, if the function is called with invalid data, the user will see a message that something is wrong. However, the if statement adds three lines of code to the function. That may not seem like much, but it clutters the code and, in a typical program with several functions, those if statements will start to feel like undesirable baggage. There’s a better way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e20912",
   "metadata": {},
   "source": [
    "### HTT-20.2.2. The `assert` Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32941054",
   "metadata": {},
   "source": [
    "Python provides a statement called the `assert` statement that can be used to check function preconditions. An assert statement checks the value of a boolean expression. If the expression is `True`, the `assert` statement allows the program to proceed normally. But if the expression is `False`, the `assert` statement signals an error and stops the program.\n",
    "\n",
    "Here’s an example of an `assert` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a844f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# htt20_2_3-ac_assert_1.py\n",
    "\n",
    "x = 1 + 1\n",
    "assert x == 2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc246bca",
   "metadata": {},
   "source": [
    "To see it in action, run the example above. You’ll see the value `2` displayed. The boolean condition `x == 2` was `True`, and the `assert` statement allowed execution to continue.\n",
    "\n",
    "Try changing the `assert` statement above as follows:\n",
    "\n",
    "`assert x == 3`\n",
    "\n",
    "Run this version of the code, and you’ll see an `AssertionError` appear. That occurred because the value of the boolean expression was `False`.\n",
    "\n",
    "Let’s modify our `sumnums` function to use an `assert` statement to check the precondition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ad3975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(sumnums(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msumnums\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36msumnums\u001b[0;34m(lo, hi)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msumnums\u001b[39m(lo, hi):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"returns the sum of the numbers in the range [lo..hi]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Precondition: lo <= hi\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m lo \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m hi\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(lo, hi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# htt20_2_4-ac_sumnums_3.py\n",
    "\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"returns the sum of the numbers in the range [lo..hi]\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    \"\"\"\n",
    "\n",
    "    assert lo <= hi\n",
    "\n",
    "    sum = 0\n",
    "    for i in range(lo, hi+1):\n",
    "        sum += i\n",
    "    return sum\n",
    "\n",
    "print(sumnums(1, 3))\n",
    "print(sumnums(3, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e15898",
   "metadata": {},
   "source": [
    "In this version of `sumnums`, we’ve replaced the `if` statement with an `assert` statement. Notice that the boolean condition of the assert statement is the precondition, `lo <= hi`. When the function is called, if the condition is true, the function completes normally and returns its result. If the condition is `False`, the program stops with an `AssertionError`. So, the first call to `sumnums(1, 3)` succeeds and the result, `6`, appears. The second call to `sumnums(3, 1)` causes the assert to fail and an error appears.\n",
    "\n",
    "Notice how much more streamlined this version of the function is than the version with the three-line `if` statement. Here, we’ve added just one line of code to the original version. Using assertions is a relatively low-effort way to create defensive functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac20d2d",
   "metadata": {},
   "source": [
    "***Writing assert statements to check preconditions***\n",
    "\n",
    "Writing `assert` statements to check preconditions is easy. They go at the **beginning** of the function. When you write an `assert` statement to check a precondition, if the function comment already contains a precondition, you often can simply take the precondition and put it directly into the `assert` statement (you might have to tweak it to make it syntactically legal). If there is no precondition in the function comment, think about how you would write an if statement to check that the values in the parameters are **correct**, and then put that condition after the word `assert`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a0c45",
   "metadata": {},
   "source": [
    "### HTT-20.2.3 - More on `assert` and Preconditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c1de0",
   "metadata": {},
   "source": [
    "Let’s discuss for a moment the question of what a defensive function should do when it receives invalid values in its parameters. By using an `assert` statement to check preconditions, we’ve designed the function to terminate the program if it is given bad data. Is this the right thing to do? If the program ends abruptly due to an assertion failure, the user will lose whatever work is in progress. That seems undesirable, to put it mildly.\n",
    "\n",
    "Although a full discussion of defensive programming and assertions is outside the scope of an introductory programming textbook, think about this: an assertion error ***indicates a bug in the program***. More specifically, the bug is a logic error that resulted in calling a function with inappropriate parameter values. If a computation is in progress and a logic error occurs, any results that computation might produce will be faulty. Logic errors often go silently undetected by users, because they aren’t aware that the output is incorrect. It is better for a user to lose work than for a logic error to go undetected and produce an invalid result that might be unwittingly used. Therefore, using `assert` statements to check function preconditions is entirely appropriate.\n",
    "\n",
    "Not only will adding assertions to your functions to check preconditions help expose logic errors in your program, it does so in a way that helps you track them down and fix them quickly. When you don’t use assertions, a function that is called with incorrect parameters may produce erroneous results that aren’t detected until much later in the program, and debugging the problem can be difficult to trace back to the source. When you use assertions to check preconditions, a function that detects a problem will stop immediately, helping you pinpoint the problem much faster. This behavior is called the ***fail fast principle***. You want your program to fail as quickly after a logic error is detected as possible to help streamline the diagnostic work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b650d8",
   "metadata": {},
   "source": [
    "***Debugging Assertion Failures***\n",
    "\n",
    "When an assert statement that you have written to check a function precondition signals an error at runtime, your first thought will probably be: “what went wrong? where’s the problem?” It will help if you remember that an assert that checks a function precondition is there to catch bugs in code that calls the function. After all, you put it on the first line of the function. So, it’s not an indication of a problem in the function: instead, the calling code has a problem. So, look to see what code called the function. When you’re running your program in a regular Python interpreter, the full error message will show the exact sequence of calls that triggered the error, and you can tell exactly which line of code is responsible for providing the incorrect values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b4c84",
   "metadata": {},
   "source": [
    "***Functions that Cannot Fail***\n",
    "\n",
    "An alternative approach to handling bad input for sumnums would be to design the function so that it works correctly regardless of whether the low end of the range is specified first or second. For example, we could design it so that both of the following calls produce correct results:\n",
    "\n",
    "```\n",
    "print(sumnums(1, 3))\n",
    "print(sumnums(3, 1))\n",
    "```\n",
    "\n",
    "It’s not hard to do; I bet you could figure out how to tweak the function to work correctly for both of these calls without much effort. However, a more important question is: should we do that?\n",
    "\n",
    "This question doesn’t necessarily have a simple answer, but briefly, there are a couple of considerations that argue against it. First, refining the function to work correctly for both of these calls will result in a function that is slightly more complex, and therefore, perhaps more likely to contain bugs. Also, testing will be more involved; there are more cases to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589c205",
   "metadata": {},
   "source": [
    "***Check your understanding***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff254a2",
   "metadata": {},
   "source": [
    "An `assert` statement displays output if the condition is `True`.\n",
    "\n",
    "A. `True`  \n",
    "B. `False` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3568ca5",
   "metadata": {},
   "source": [
    "***Check your understanding***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc37833",
   "metadata": {},
   "source": [
    "Consider the following function. Which `assert` should be added to check its precondition?\n",
    "\n",
    "```\n",
    "def getfirst(msg):\n",
    "    \"\"\"returns first character of msg\n",
    "\n",
    "    Precondition: len(msg) > 0\n",
    "    \"\"\"\n",
    "\n",
    "    return msg[0]\n",
    "\n",
    "A. assert len(msg) <= 0\n",
    "B. assert len(msg) > 0\n",
    "C. assert msg[0]\n",
    "D. none of these\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae909b6b",
   "metadata": {},
   "source": [
    "## HTT-20.3 - Testing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc950182",
   "metadata": {},
   "source": [
    "Melinda is writing a program that does some mathematical calculations. At the moment, she is working on adding some functionality to her program that requires rounding numbers to the nearest integer. She would normally use the built-in Python function `round` to do the job, but her program has a special requirement that numbers should be rounded up if the fractional portion is .6 or greater, instead of the usual .5 or greater. So, Melinda decides to write a function that rounds up numbers according to this requirement.\n",
    "\n",
    "She defines a function `round6` to do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c0b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round6(num):\n",
    "    \"\"\"returns num rounded to nearest int if fractional part is >= .6\"\"\"\n",
    "\n",
    "    return int(num + .6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff2ab2",
   "metadata": {},
   "source": [
    "This function uses a valid approach to rounding, but is not quite correct (Melinda doesn’t realize it yet — can you spot the bug?).\n",
    "\n",
    "Now she needs to test the new code. There are two basic approaches Melinda could take to do her testing:\n",
    "\n",
    "1. Put the function into the program, modify the program to call the function at the appropriate point, then run the program.\n",
    "\n",
    "2. Test the function by itself, somehow.\n",
    "\n",
    "Which do you think will be more efficient?\n",
    "\n",
    "Melinda’s program does complex mathematical calculations, and asks the user to enter 5 separate pieces of input before performing the calculations. If she goes with option 1, each time she runs the program to test the function, she must enter all 5 pieces of input. As you can imagine, that process is cumbersome and will not be very efficient. Also, if the program output is incorrect, it may be difficult to determine whether the fault is in the new function, or elsewhere in the program.\n",
    "\n",
    "Melinda decides to write a separate, short program to help her test her new function. The test program is very simple — it contains only her new function and a bit of code to get some input, pass it to the function, and display the result. Here’s what she writes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ffa731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a number:7\n",
      "Result:  7\n"
     ]
    }
   ],
   "source": [
    "# htt_20_3_1-ac_round6_1.py\n",
    "\n",
    "def round6(num):\n",
    "    \"\"\"returns num rounded to nearest int if fractional part is >= .6\"\"\"\n",
    "\n",
    "    return int(num + .6)\n",
    "\n",
    "# ----- test program -------\n",
    "\n",
    "x = float(input('Enter a number:'))\n",
    "result = round6(x)\n",
    "print('Result: ', result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6864f",
   "metadata": {},
   "source": [
    "Before running the program, she jots down some test cases to help her in her testing:\n",
    "\n",
    "```\n",
    "              Input    Expected Output\n",
    "              -------- ---------------\n",
    "Test Case 1:       3.5               3\n",
    "Test Case 2:       3.6               4\n",
    "Test Case 3:       3.7               4\n",
    "```\n",
    "\n",
    "Try running the program with the input values above. Notice that the output isn’t quite right. Can you figure out how to correct the bug?\n",
    "\n",
    "After analyzing her logic, Melinda corrects the bug by changing the return statement in the function as follows:\n",
    "\n",
    "```\n",
    "return int(num + .4)\n",
    "```\n",
    "\n",
    "She runs the test program again to verify that the function is working correctly. Then, she copies the `round6` function into her main program, confident that her rounding logic is correct.\n",
    "\n",
    "The program Melinda wrote to help her test her `round6` function is an example of a **unit test**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47b370",
   "metadata": {},
   "source": [
    "***Unit Test***\n",
    "\n",
    "A **unit test** is code that tests a function to determine if it works properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bd625",
   "metadata": {},
   "source": [
    "A unit test program like this one can dramatically reduce the effort it takes to test a new function, and can reduce the overall effort involved in adding functionality to a program. The savings tradeoff depends on the amount of effort required to write the test program, compared to the amount of effort required to test the function in the context of the main program for which the new function is being developed. Here, the function was relatively simple, and it probably wouldn’t have taken Melinda too many iterations of testing the function in the context of the main program, with its five pieces of input. In this scenario, Melinda may not have saved much effort. However, if the function were more complex, writing a unit test would probably have helped reduce the overall effort. And, using some tricks I’ll show you in the next sections, you can reduce the amount of effort required to write and run the unit test, making the case for writing unit tests even more compelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145fb4e",
   "metadata": {},
   "source": [
    "### HTTP-20.3.1 - Automated Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d1755",
   "metadata": {},
   "source": [
    "The unit test program above is a manual unit test. A manual unit test gets input from the user, invokes the code under test, providing the input supplied by the user, and displays the result. (In our example, `round6` is the code under test.) Manual unit tests are helpful, but they can be improved in two ways:\n",
    "\n",
    "1. We can embed the test input directly within the unit test code, so the person running the test doesn’t have to come up with the test input or take the time to enter it.\n",
    "\n",
    "2. We can make the unit test report success or failure, instead of requiring the person running the test to look at the output and determine whether the function worked correctly.\n",
    "\n",
    "We call a unit test that contains its own test input and produces a clear pass/fail indication an automated unit test. Take a look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "605fc7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: PASS\n",
      "Test 2: PASS\n"
     ]
    }
   ],
   "source": [
    "# htt_20_3_1-ac_round6_2.py\n",
    "\n",
    "# def round6(num):\n",
    "#    return int(num + .4)\n",
    "\n",
    "# ---- automated unit test ----\n",
    "\n",
    "result = round6(9.7)\n",
    "if result == 10:\n",
    "    print(\"Test 1: PASS\")\n",
    "else:\n",
    "    print(\"Test 1: FAIL\")\n",
    "\n",
    "result = round6(8.5)\n",
    "if result == 8:\n",
    "    print(\"Test 2: PASS\")\n",
    "else:\n",
    "    print(\"Test 2: FAIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5f29c",
   "metadata": {},
   "source": [
    "This automated unit test invokes the `round6` function on predetermined test input, checks that the function produced the expected result, and displays a pass / fail message. Run it to see the test `PASS` messages.\n",
    "\n",
    "Try editing the `round6` function above to introduce Melinda’s original bug, then run it again to see the failure message. Notice the big advantage of an automated unit test: you can change the function being tested, run the unit test, and immediately see the test results for a whole series of tests. No hand-entry of test data, and no interpretation of the results. Clearly, once you have the test written, you can dramatically speed up your edit-test-debug cycle. The downside, of course, is that the unit test program itself takes more time to develop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415f016",
   "metadata": {},
   "source": [
    "### HTTP-20.3.2 - Automated Unit Tests with `assert`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20afa44",
   "metadata": {},
   "source": [
    "To help reduce the amount of effort required to develop an automated unit test, let’s bring the `assert` statement into play. We can replace each if statement in the program above with an `assert`, as in the program below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42288830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: PASS\n",
      "Test 2: PASS\n"
     ]
    }
   ],
   "source": [
    "# htt_20_3_1-ac_round6_3.py\n",
    "\n",
    "def round6(num):\n",
    "    return int(num + .4)\n",
    "\n",
    "# ---- automated unit test ----\n",
    "\n",
    "result = round6(9.7)\n",
    "if result == 10:\n",
    "    print(\"Test 1: PASS\")\n",
    "else:\n",
    "    print(\"Test 1: FAIL\")\n",
    "\n",
    "result = round6(8.5)\n",
    "if result == 8:\n",
    "    print(\"Test 2: PASS\")\n",
    "else:\n",
    "    print(\"Test 2: FAIL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b95dcf",
   "metadata": {},
   "source": [
    "Try running the program above to see the success message. Then, try altering the `round6` function to reintroduce the original bug, and see how the assertion failure pinpoints that the second test failed.\n",
    "\n",
    "We can streamline this program even further by eliminating the result variable:\n",
    "\n",
    "```\n",
    "assert round6(9.7) == 10\n",
    "assert round6(8.5) == 8\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "```\n",
    "\n",
    "This is Really Nice. We have a short test program that contains its own test input and displays an automated pass or fail indication. Writing this program takes very little effort. We have the benefits of an automated test without having to write much code. Unit test programs are essentially “throw-away” programs that are used only during development, and it’s important that they can be developed quickly and easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2db4c6",
   "metadata": {},
   "source": [
    "### HTT-20.3.3 - Unit Tests can have bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474edb4a",
   "metadata": {},
   "source": [
    "Unit tests, like the functions they test, can have bugs. So, when you run a unit test and it fails with an assert error, one of the first questions you need to ask yourself is: “Is the unit test correct?” If the unit test is incorrect, then you need to correct it, rather than spending time trying to find the bug in the function that the unit test is testing.\n",
    "\n",
    "For example, consider the following assert:\n",
    "\n",
    "`assert round6(9.2) == 10`\n",
    "\n",
    "This unit test is incorrect, because `round6` should produce the value `9`, not `10`, when given the parameter `9.2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285904bf",
   "metadata": {},
   "source": [
    "***Check your understanding***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75184bd2",
   "metadata": {},
   "source": [
    "Rewrite the following 3 lines of code with a single `assert`:\n",
    "\n",
    "```\n",
    "result = engage_thruster(22)\n",
    "if result != 'OK':\n",
    "    print(\"Test 2: FAIL\")\n",
    "\n",
    "A. assert result != 'OK'\n",
    "B. assert engage_thruster(22) == result\n",
    "C. assert engage_thruster(22) != 'OK'\n",
    "D. assert engage_thruster(22) == 'OK'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967ff96",
   "metadata": {},
   "source": [
    "***Check your understanding***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb699b9d",
   "metadata": {},
   "source": [
    "Consider the following function which is supposed to return the first character of its argument:\n",
    "\n",
    "```\n",
    "def get_first(msg):\n",
    "    return msg[1]\n",
    "```\n",
    "\n",
    "Now, consider this unit test:\n",
    "\n",
    "`assert get_first('Bells') == 'B'`\n",
    "\n",
    "This assertion fails. Is the unit test in error, or the function it is testing?\n",
    "\n",
    "```\n",
    "A. Unit test\n",
    "B. Tested function\n",
    "C. Both are in error\n",
    "D. Both are correct\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353503c8",
   "metadata": {},
   "source": [
    "## HTT-20.4 -  Designing Testable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6ebb4",
   "metadata": {},
   "source": [
    "Now that you know how to write unit tests using the `assert` statement, it’s important for you to understand how to write testable functions. Not all functions can be tested.\n",
    "\n",
    "Consider the following function:\n",
    "\n",
    "```\n",
    "def add(x, y):\n",
    "    \"\"\"Adds two numbers and displays the sum\"\"\"\n",
    "    print(x + y)\n",
    "```\n",
    "\n",
    "How would you write an assert statement to check that this function works? Think about it a moment. Would this work?\n",
    "\n",
    "`assert add(2, 3) == 5`\n",
    "\n",
    "Answer: no. An `assert` statement cannot verify that what a function displays on the screen is correct. It can only check that the contents of variables are correct. This function is not testable.\n",
    "\n",
    "A ***testable function*** is a function that produces a result that can be checked by an assert statement. Generally, it does so in one of three ways:\n",
    "\n",
    "1. It returns its result\n",
    "\n",
    "2. It stores its result in a global variable\n",
    "\n",
    "3. It modifies the state of an object passed as a parameter\n",
    "\n",
    "Functions that display their output using `print` are ***not*** testable functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129eafc6",
   "metadata": {},
   "source": [
    "***Check your understanding***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eaf402",
   "metadata": {},
   "source": [
    "Is this a testable function?\n",
    "\n",
    "```\n",
    "sum = 0\n",
    "def add(x, y):\n",
    "    global sum\n",
    "    sum = x + y\n",
    "```\n",
    "\n",
    "```\n",
    "A. Yes.\n",
    "B. No.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa1386",
   "metadata": {},
   "source": [
    "### HTT-20.4.1 - Design by Contract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44230d67",
   "metadata": {},
   "source": [
    "In addition to producing a result that can be checked by an assert statement, a testable function must have a clear specification. In order to write unit tests for a function, you must have a precise understanding of what the function should do.\n",
    "\n",
    "A function specification describes what value the function produces, given its parameter values, and is generally expressed in the form of a docstring. For example, consider the `sumnums` function given earlier in this chapter:\n",
    "\n",
    "```\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"returns the sum of the numbers in the range [lo..hi]\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    \"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "The docstring is this function’s specification. Given this specification, you might write a unit test that contains the following `assert`:\n",
    "\n",
    "`assert sumnums(1, 3) == 6`\n",
    "\n",
    "An alternate way to write the docstring is as follows:\n",
    "\n",
    "```\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"computes the sum of a range of numbers\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    Postcondition: returns the sum of the numbers in the range [lo..hi]\n",
    "    \"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "This docstring contains three elements: a brief description; a precondition; and a postcondition. We’ve discussed the concept of a precondition earlier in this chapter. The postcondition is new."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a392eb8",
   "metadata": {},
   "source": [
    "**Postcondition**\n",
    "\n",
    "A **postcondition** states the work the function completed by the function if the precondition is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5b5dc",
   "metadata": {},
   "source": [
    "Functions that include a precondition and a postcondition in their docstring embody a software engineering idea called ***design by contract***. The idea is that a function specification forms a contract between the function and the code calling the function. If the code calling the function passes parameters that satisfy the function’s precondition, then the function should be expected to produce what it says it will produce. If the parameters do not satisfy the function’s precondition, then the function does not have to produce a valid result. In the design by contract approach, a testable function is one where the function’s postcondition can be verified by an `assert` statement.\n",
    "\n",
    "In this example, you can think of the function’s docstring as promising to calling code: “If you give me two parameters, `lo` and `hi`, such that `lo` is less than or equal to `hi`, I promise to return the sum of the numbers in the range `lo..hi`, inclusive.”\n",
    "\n",
    "To write a precondition, think about the parameter values that the function is designed to handle, and write a boolean expression that expresses what parameter values are valid. For example, consider a function that computes the average weight, given a total weight and a number of items:\n",
    "\n",
    "```\n",
    "def compute_average(total_weight: float, num_items: float) -> float:\n",
    "    return total / num_items\n",
    "```\n",
    "\n",
    "This function will work if `num_items` is greater than zero, but will fail if `num_items` is zero. So, an appropriate precondition would be `num_items > 0`. A complete docstring would look like this:\n",
    "\n",
    "```\n",
    "def compute_average(total_weight: float, num_items: float) -> float:\n",
    "    \"\"\"computes the average weight, given `total_weight` of items and `num_items`\n",
    "\n",
    "    Precondition: num_items > 0\n",
    "    Postcondition: returns average item weight\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Sometimes, your precondition will be expressed more loosely, using English. Consider this function which extracts the first word from a string containing text:\n",
    "\n",
    "```\n",
    "def get_first_word(text: str) -> str:\n",
    "    \"\"\"extracts the first word from `text`\"\"\"\n",
    "\n",
    "    space_loc = text.find(' ')\n",
    "    return text[0:space_loc]\n",
    "```\n",
    "\n",
    "This function will produce nonsense if the string doesn’t contain a space. So, an appropriate precondition might be “text contains 2 or more words separated by spaces”. The docstring might be:\n",
    "\n",
    "```\n",
    "def get_first_word(text: str) -> str:\n",
    "    \"\"\"extracts the first word from `text`\n",
    "\n",
    "    Precondition: `text` contains 2 or more words separated by spaces\n",
    "    Postcondition: returns the first word in `text`\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Following the design by contract idea and writing function specifications that include preconditions and postconditions is an excellent way to design testable functions, because, as we’ll see in the next section, it makes it possible to reason precisely about what the function should do when given various parameter values. Even if you don’t use precondition and postcondition terminology in your docstrings, it helps to think in those terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54131045",
   "metadata": {},
   "source": [
    "**Check your understanding**\n",
    "\n",
    "Consider the following function. What would an appropriate precondition be?\n",
    "\n",
    "```\n",
    "def getfirst(msg):\n",
    "    \"\"\"returns first character of msg\"\"\"\n",
    "\n",
    "    return msg[0]\n",
    "```\n",
    "```\n",
    "A. len(msg) <= 0 \n",
    "B. len(msg) > 0 \n",
    "C. msg == \"\" \n",
    "D. none of these \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1d2af",
   "metadata": {},
   "source": [
    "## HTT-20.5 - Writing Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a1449",
   "metadata": {},
   "source": [
    "Once you have designed a testable function, with a clear docstring specification, writing unit tests is not difficult. In this section, you’ll learn how to do just that.\n",
    "\n",
    "Let’s start with our `sumnums` function:\n",
    "\n",
    "```\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"computes the sum of a range of numbers\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    Postcondition: returns the sum of the numbers in the range [lo..hi]\n",
    "    \"\"\"\n",
    "\n",
    "    sum = 0\n",
    "    for i in range(lo, hi+1):\n",
    "        sum += i\n",
    "    return sum\n",
    "```\n",
    "\n",
    "As we’ve seen, to write a unit test, you devise test cases for the function, and then write assert statements that call the function and check that the function produced the expected results. The following assert statements would be appropriate for a unit test for sumnums:\n",
    "\n",
    "```\n",
    "assert sumnums(1, 3) == 6\n",
    "assert sumnums(1, 1) == 1\n",
    "```\n",
    "\n",
    "But what about the following?\n",
    "\n",
    "`assert sumnums(3, 1) == 0`\n",
    "\n",
    "Note that `sumnums` produces the value `0` for cases where the `lo` values exceeds the `hi` value, as is the case in this `assert`. So, like the first two `assert`s above, this `assert` would pass. However, it is not an appropriate assertion, because the specification says nothing about what the function produces if `lo` is greater than `hi`.\n",
    "\n",
    "The unit test should be written such that it passes even if the function implementation is altered in a way that causes some other value than 0 to be returned if `lo` exceeds `hi`. For example, we might want to redesign the function to be more efficient — for example, use Gauss’s formula for summing numbers, as in the following:\n",
    "\n",
    "```\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"computes the sum of a range of numbers\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    Postcondition: returns the sum of the numbers in the range [lo..hi]\n",
    "    \"\"\"\n",
    "\n",
    "    return (hi * (hi + 1) / 2) - (lo * (lo - 1) / 2)\n",
    "```\n",
    "This version will produce correct results if the precondition is satisfied. Like the original function, it produces incorrect results if the precondition is violated — but unlike the original function, the values produced if the precondition is violated are not necessarily 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cd23a",
   "metadata": {},
   "source": [
    "### HTT-20.5.1 - Specification-Based Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16334c98",
   "metadata": {},
   "source": [
    "A key idea to remember when writing a unit test is that your test must always respect the function’s preconditions. The docstring states what the function should do, with the assumption that parameter values meet the preconditions. It does not state what the function should do if the parameter values violate the preconditions.\n",
    "\n",
    "Writing an assert that violates the functions preconditions is not a good idea, because to determine what the function will produce for that case, you must look into the implementation of the function and analyze its behavior. That is called implementation-based testing, and it leads to brittle tests that are likely to fail if you rework the function implementation. When you write tests are based only on the function specification, without looking at the implementation, you are doing specification-based testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c05c8",
   "metadata": {},
   "source": [
    "**Specification-Based Tests**\n",
    "\n",
    "Specification-based tests are tests that are designed based only on the information in the function specification, without considering any of the details in the function implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e2b9d",
   "metadata": {},
   "source": [
    "Specification-based tests are preferred over implementation-based tests, because they are more resilient. They will continue to pass even if you rework the function implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729c0d81",
   "metadata": {},
   "source": [
    "**Check your understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c773d",
   "metadata": {},
   "source": [
    "Consider the following function. Indicate which of the asserts would be appropriate for a unit test.\n",
    "\n",
    "```\n",
    "def repeat(s: str, num: int) -> str:\n",
    "    \"\"\"duplicates a string\n",
    "\n",
    "    Precondition: `num` >= 0\n",
    "    Postcondition: Returns a string containing `num` copies of `s`\n",
    "    \"\"\"\n",
    "    if num >= 0:\n",
    "        return s * num\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "A. assert repeat('*', 0) == ''\n",
    "B. assert repeat('*', -1) == ''\n",
    "C. assert repeat('-', 5) == '-----'\n",
    "D. assert repeat('*', 5) == '***'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb8ec2",
   "metadata": {},
   "source": [
    "**Check your understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd5a02",
   "metadata": {},
   "source": [
    "Write `assert` statements below to test a function with the following specification. Your `assert`s should check that the function produces an appropriate value for each of the three postcondition cases.\n",
    "\n",
    "```\n",
    "def grade(score):\n",
    "    \"\"\"Determines letter grade given a numeric score\n",
    "\n",
    "    Precondition: 0 <= score <= 100\n",
    "    Postcondition: Returns 'A' if 90 <= score <= 100,\n",
    "      'B' if 80 <= score < 90, 'F' if 0 <= score < 80\n",
    "    \"\"\"\n",
    "```\n",
    "Note: Line numbers in any assert error messages that appear while you are developing and testing your answer will not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "663bcf9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grade' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write assert statements to test grade()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mgrade\u001b[49m(\u001b[38;5;241m95\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m grade(\u001b[38;5;241m85\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m grade(\u001b[38;5;241m47\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grade' is not defined"
     ]
    }
   ],
   "source": [
    "# Write assert statements to test grade()\n",
    "\n",
    "assert grade(95) == 'A'\n",
    "assert grade(85) == 'B'\n",
    "assert grade(47) == 'F'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ee6da",
   "metadata": {},
   "source": [
    "## 20.6. Test-First Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc542c83",
   "metadata": {},
   "source": [
    "The idea of unit tests has been around a long time, and most people agree that writing unit tests is a good idea. However, when deadlines loom and time is at a premium, the unit tests often don’t get written. That’s a problem, because studies have shown that projects with good unit tests often are more robust, with fewer bugs, than projects that don’t have good unit tests.\n",
    "\n",
    "In a traditional development process, when a programmer needs to create a new function, he writes the function, and then, if he has time, writes a unit test for it. If he doesn’t have time, he doesn’t write the unit test: he tests the function in the context of the program being developed. One day, someone decided that it might be a good idea to reverse the order: write the unit test first, and then write the function. That led to the idea of Test-First Development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b964f9",
   "metadata": {},
   "source": [
    "**Test-First Development**\n",
    "\n",
    "**Test-First Development** is an approach to writing software that involves writing a unit test for a function ***before*** writing the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bc074",
   "metadata": {},
   "source": [
    "In this section, we’ll explore the idea of test-first development to see how it can help.\n",
    "\n",
    "A programmer using Test-First Development writes a new function using the following steps:\n",
    "\n",
    "1. First, create the function interface and docstring.\n",
    "\n",
    "2. Next, create a unit test for the function.\n",
    "\n",
    "3. Run the unit test. It should fail.\n",
    "\n",
    "4. Write the body of the function.\n",
    "\n",
    "5. Run the unit test. If it fails, debug the function, and run the test again. Repeat until the test passes.\n",
    "\n",
    "As an example, suppose that we’re going to write our sumnums function using the Test-First methodology. We begin by creating the interface and docstring:\n",
    "\n",
    "```\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"computes the sum of a range of numbers\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    Postcondition: returns the sum of the numbers in the range [lo..hi]\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Next, we write the unit test for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46e0b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"computes the sum of a range of numbers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Precondition: lo <= hi\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Postcondition: returns the sum of the numbers in the range [lo..hi]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sumnums(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sumnums(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ac_tfd_sumnums.py\n",
    "\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"computes the sum of a range of numbers\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    Postcondition: returns the sum of the numbers in the range [lo..hi]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "assert sumnums(1, 3) == 6\n",
    "assert sumnums(1, 1) == 1\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf02a6",
   "metadata": {},
   "source": [
    "We run the unit test and it fails.\n",
    "\n",
    "Next, we implement the body of `sumnums`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e76e122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# ac_tfd_sumnums2.py\n",
    "\n",
    "def sumnums(lo, hi):\n",
    "    \"\"\"computes the sum of a range of numbers\n",
    "\n",
    "    Precondition: lo <= hi\n",
    "    Postcondition: returns the sum of the numbers in the range [lo..hi]\n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    for i in range(lo, hi+1):\n",
    "        sum += i\n",
    "    return sum\n",
    "\n",
    "assert sumnums(1, 3) == 6\n",
    "assert sumnums(1, 1) == 1\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6261d",
   "metadata": {},
   "source": [
    "Now, run the tests. The tests indicate an assertion error, which points to a bug in the function logic. Fix the bug, and test again. (If you’re not sure what the bug is, try using ```Show in CodeLens``` and stepping through the code to help you figure it out:\n",
    "\n",
    "https://runestone.academy/ns/books/published/thinkcspy/UnitTesting/TestFirstDevelopment.html?mode=browsing#ac_tfd_sumnums2\n",
    "\n",
    "Suppose we’re not creating a new function, but modifying an existing one. In Test-First Development, before making the modification to the function, we write a test for the new functionality. Then, we modify the function, and use the test to check that the modification worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c33f8",
   "metadata": {},
   "source": [
    "### HTT-20.6.1 - Benefits of Test-First Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296734f",
   "metadata": {},
   "source": [
    "There are several benefits to Test-First Development.\n",
    "\n",
    "1. It ensures that unit tests are written. This tends to lead to higher-quality, robust code, with fewer bugs.\n",
    "\n",
    "1. Writing the tests first helps the programmer to clarify the function specification. It’s not possible to write an assert for a function that has a vague function docstring. This process forces the programmer to write a clear docstring and to practice specification-based testing, because when the tests are written, there is no function implementation to reference.\n",
    "\n",
    "1. When the programmer writes the function and is ready to test it, the test is all ready to go. There is no internal struggle about whether a unit test should be written or not. The programmer runs the test, and gets instant feedback about whether the function is working or not.\n",
    "\n",
    "1. If the function fails to pass the test, the benefits of unit testing in helping the programmer to quickly diagnose and fix the problem are instantly available. The test-debug cycle is rapid.\n",
    "\n",
    "1. When a programmer modifies an existing function for which unit tests already exist, perhaps to add some more functionality, the existing unit tests serve as a safety net. They check that the modifications made by the programmer don’t break any of the old functionality.\n",
    "\n",
    "1. The overall development time tends to be reduced. Perhaps counter-intuitively, writing more code (the unit tests) actually speeds up the overall development process, because of the benefits imparted by unit testing.\n",
    "\n",
    "1. Believe it or not, there are psychological benefits. As the programmer works on the project, creating little tests and then writing code that passes those tests, there is a sense of accomplishment and satisfaction that comes every time a new test passes. Instead of spending hours of frustration debugging a new function in the context of a complex program, with few visible results, the test-first progress leads to more visible and regular successes.\n",
    "\n",
    "I hope you’ll try out Test-First Development on your next assignment and experience some of these benefits for yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6e60c",
   "metadata": {},
   "source": [
    "**Check your understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290d178",
   "metadata": {},
   "source": [
    "Test-First Development often involves writing more code than traditional development.\n",
    "```\n",
    "A. True\n",
    "B. False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9edddd",
   "metadata": {},
   "source": [
    "## HTT-20.7 - Testing with `pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf3091",
   "metadata": {},
   "source": [
    "Writing automated unit tests is very helpful in reducing the effort needed to build software. However, the simple approach described so far is inadequate to help programmers realize the full benefits of unit testing. In this section, we introduce a unit test framework which addresses some practical issues that crop up when you try to apply unit testing techniques in software development projects. Here are some of the issues with using plain assertion unit tests:\n",
    "\n",
    "- Simple assert-based tests don’t give very good diagnostic information when they fail. It would help to have better reporting. For example, when an assert fails, it would help us in diagnosing the error to see what value the function actually produced. An `AssertionError` doesn’t give us that information.\n",
    "\n",
    "- We need a better way to organize our unit test code. So far, I’ve suggested creating separate programs to hold the functions under test together with their unit test code. But that isn’t practical for most projects. For example, functions often need to call other functions in the program in order to do their work. It’s not convenient to bring those auxiliary functions over into separate test programs.\n",
    "\n",
    "- We need a way to keep the unit test around and use it even after the function is first developed to help us catch bugs that are inadvertently introduced into the function when we make modifications to it.\n",
    "\n",
    "Unit testing frameworks help to address these issues, by improving error reporting, providing a structure for programmers to organize their unit tests, and making it possible to leverage existing unit tests when making enhancements to functions. **`pytest`** is one unit testing framework that provides these benefits.\n",
    "\n",
    "For our purposes, the attractive thing about `pytest` is that writing unit tests with `pytest` feels a lot like writing unit tests using plain `assert` statements. The only difference is that you put your `assert` statements into test functions. Here’s an example of how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03fe3c",
   "metadata": {},
   "source": [
    "Run the following in the book's Active Code window (simulating `pytest`):\n",
    "    \n",
    "https://runestone.academy/ns/books/published/thinkcspy/UnitTesting/TestingWithpytest.html?mode=browsing#ac-round6-pytest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36551757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_htt20_7_1-round6.py\n",
    "\n",
    "# pytest runs files with prefix test_:\n",
    "#   each function with prefix test_ then is executed as part of the test\n",
    "\n",
    "def round6(num: float) -> int:\n",
    "    \"\"\"This function has a bug in it\"\"\"\n",
    "    return int(num + .6)\n",
    "\n",
    "# ---- automated unit test ----\n",
    "\n",
    "def test_round6():\n",
    "    assert round6(9.7) == 10\n",
    "    assert round6(8.5) == 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d007c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "  Downloading pytest-7.2.2-py3-none-any.whl (317 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /Users/tarunsingh/Downloads/yes/lib/python3.9/site-packages (from pytest) (22.2.0)\n",
      "Requirement already satisfied: packaging in /Users/tarunsingh/Downloads/yes/lib/python3.9/site-packages (from pytest) (23.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /Users/tarunsingh/Downloads/yes/lib/python3.9/site-packages (from pytest) (1.0.0)\n",
      "Installing collected packages: tomli, iniconfig, exceptiongroup, pytest\n",
      "Successfully installed exceptiongroup-1.1.1 iniconfig-2.0.0 pytest-7.2.2 tomli-2.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01a55b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.15, pytest-7.2.2, pluggy-1.0.0\n",
      "rootdir: /Users/tarunsingh/aiml/St Thomas/SEIS 604 - PYTHON/Labs/tsingh_class_6\n",
      "plugins: anyio-3.6.2\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_htt_20_7_1-round6.py \u001b[31mF\u001b[0m\u001b[31m                                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_round6 __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_round6\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m round6(\u001b[94m9.7\u001b[39;49;00m) == \u001b[94m10\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m round6(\u001b[94m8.5\u001b[39;49;00m) == \u001b[94m8\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 9 == 8\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 9 = round6(8.5)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_htt_20_7_1-round6.py\u001b[0m:15: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_htt_20_7_1-round6.py::\u001b[1mtest_round6\u001b[0m - assert 9 == 8\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# you can run it within this notebook:\n",
    "# assumes .py in a separate file that is directly within current project folder\n",
    "\n",
    "import pytest\n",
    "! pytest test_htt_20_7_1-round6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15919e7c",
   "metadata": {},
   "source": [
    "***You can also run the previous pytest tests within PyCharm: we'll demonstrate...***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b276be5",
   "metadata": {},
   "source": [
    "This code example defines two functions: the function to be tested, `round6`, and a function named `test_round6` that contains unit test code. When using the pytest approach, you write your unit test as a function whose name must start with the prefix `test_`. Inside the function, you write normal assert statements to test the desired function. Notice that you do not write a line to call the unit test function. Instead, when you launch `pytest` to run the unit tests, `pytest` scans your script and executes only the functions with the prefix `test_`.\n",
    "\n",
    "(**For the interactive book only**) This ActiveCode environment simulates `pytest` by scanning for and executing functions with a `test_` prefix when you click `Run`. Go ahead and try it - rename the `test_round6` function to `test_itworks` and try running the test again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d0422",
   "metadata": {},
   "source": [
    "### HTT-20.7.1. Organizing `pytest` Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd812d5b",
   "metadata": {},
   "source": [
    "The example above uses a single `pytest` function, with both `assert` statements in the same `pytest` function. The disadvantage of that approach is that the first failing `assert` prevents the rest of the `assert` statements from being tested.\n",
    "\n",
    "If you want, you can write multiple `pytest` functions to test a single function. That way, when an `assert` fails in one test function, the rest of the pytest functions can still run and report success or failure.\n",
    "\n",
    "You can name your `pytest` functions with names that indicate what they are testing. For example, try changing the ActiveCode example above so that it defines two test functions: one named `test_round6_rounds_up`, containing the first `assert`, and one named `test_round6_rounds_down`, containing the second `assert`. Your code should look like this:\n",
    "```\n",
    "def test_round6_rounds_up():\n",
    "    assert round6(9.7) == 10\n",
    "\n",
    "def test_round6_rounds_down():\n",
    "    assert round6(8.5) == 8\n",
    "```\n",
    "If you use good `pytest` function names, when a `pytest` function has an assertion failure, you can easily tell what the problem was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0f07f",
   "metadata": {},
   "source": [
    "### HTT-20.7.2. Using `pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dae0da",
   "metadata": {},
   "source": [
    "***Note:  the following is not needed in our Anaconda environment.  `pytest` is already installed...***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f995ee",
   "metadata": {},
   "source": [
    "To use `pytest`, you must first install it using the `pip` command. Open your computer’s command line window (not the Python interpreter) and enter the following command to install:\n",
    "\n",
    "Windows:\n",
    "\n",
    "    pip install pytest\n",
    "\n",
    "Mac/Linux:\n",
    "\n",
    "    pip3 install pytest\n",
    "\n",
    "After you have installed `pytest`, you run `pytest` unit tests from the command line window. To run `pytest` unit tests, try copying the code from the ActiveCode example above and pasting it into a Python file named (ex.) `myround.py`. Then, use the `pytest` command to run your tests by opening a command window, navigating to the folder where you stored `myround.py`, and executing the following command:\n",
    "\n",
    "```\n",
    "pytest myround.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9e3c8",
   "metadata": {},
   "source": [
    "You can try it in this notebook: `myround.py` is in this notebook's folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a1baf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.15, pytest-7.2.2, pluggy-1.0.0\n",
      "rootdir: /Users/tarunsingh/aiml/St Thomas/SEIS 604 - PYTHON/Labs/tsingh_class_6\n",
      "plugins: anyio-3.6.2\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "myround.py \u001b[31mF\u001b[0m\u001b[31m                                                             [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_round6 __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_round6\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m round6(\u001b[94m9.7\u001b[39;49;00m) == \u001b[94m10\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m round6(\u001b[94m8.5\u001b[39;49;00m) == \u001b[94m8\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 9 == 8\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 9 = round6(8.5)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mmyround.py\u001b[0m:14: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m myround.py::\u001b[1mtest_round6\u001b[0m - assert 9 == 8\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest myround.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1faae",
   "metadata": {},
   "source": [
    "### HTT-20.7.3 - Understanding `pytest` Failure Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9462b",
   "metadata": {},
   "source": [
    "When you run the `pytest` command and an assertion fails, you see a report like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c35d92",
   "metadata": {},
   "source": [
    "```\n",
    "=============================== FAILURES ================================\n",
    "______________________________ test_round6 ______________________________\n",
    "    def test_round6():\n",
    "        assert round6(9.7) == 10\n",
    ">       assert round6(8.5) == 8\n",
    "E       assert 9 == 8\n",
    "E        +  where 9 = round6(8.5)\n",
    "\n",
    "myround.py:8: AssertionError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc102a6",
   "metadata": {},
   "source": [
    "Let’s take a closer look at this report to understand what it’s telling you.\n",
    "\n",
    "1. First, notice the line with the `>` symbol:\n",
    "\n",
    "    >       assert round6(8.5) == 8\n",
    "\n",
    "    The `>` symbol points to the line with the assertion that failed. \n",
    "    \n",
    "    \n",
    "2. Next, notice the lines marked E:\n",
    "\n",
    "    ```\n",
    "    E       assert 9 == 8\n",
    "    E        +  where 9 = round6(8.5)\n",
    "    ```\n",
    "\n",
    "This indicates that the call to `round6(8.5)` returned the value `9`, instead of the value `8`. The value `9` is the actual result of the function. Knowing the value actually produced by the function can help you to troubleshoot the bug and correct the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350d88f",
   "metadata": {},
   "source": [
    "### HTT-20.7.4 - Integrated Unit Testing with `pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd652a42",
   "metadata": {},
   "source": [
    "When you use the `pytest` framework, you can include `pytest` test functions in your main program, along with the rest of your program code. This allows you to keep your tests together with the functions that they test, and you can run either your program (using the python command) or the unit tests (using the pytest command).\n",
    "\n",
    "Take a look at this example that shows a function (`round6`, containing a bug), together with a unit test function (`test_round6`), and a main program that uses `round6`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fb1dd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a value:8.4\n",
      "The value rounded is: 9\n"
     ]
    }
   ],
   "source": [
    "def round6(num: float) -> int:\n",
    "\n",
    "    return int(num + .6)\n",
    "\n",
    "# ---- automated unit test ----\n",
    "\n",
    "def test_round6():\n",
    "    assert round6(9.7) == 10\n",
    "    assert round6(8.5) == 8\n",
    "\n",
    "# ----- main program follows -----\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    num = float(input('Enter a value:'))\n",
    "\n",
    "    print('The value rounded is: ' + str(round6(num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d7450",
   "metadata": {},
   "source": [
    "Notice how the `main` program is inside the `if` statement on line 12. This `if` condition is `True` when the program is run using the `python` command, and allows the main program to execute. When the unit tests are executed using the `pytest` command, any top-level code outside a function in the python file gets executed when pytest scans the script looking for unit test functions with a `test_` prefix. The if condition is `False` in this scenario, and that prevents the main program from executing when pytest is scanning the script. If that explanation didn’t make total sense, just remember: in order for `pytest` to work correctly, any code that is part of the main program must be inside an `if` statement like the one in this example, so that it doesn’t interfere with `pytest`’s unit testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74fc4a",
   "metadata": {},
   "source": [
    "**Check your understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53feabfb",
   "metadata": {},
   "source": [
    "Write a `pytest` unit test function named `test_grade` to test a function with the following specification. Your asserts should check that the function produces an appropriate value for each of the three postcondition cases.\n",
    "\n",
    "```\n",
    "def grade(score):\n",
    "    \"\"\"Determines letter grade given a numeric score\n",
    "\n",
    "    Precondition: 0 <= `score` <= 100\n",
    "    Postcondition: Returns 'A' if 90 <= `score` <= 100,\n",
    "      'B' if 80 <= `score` < 90, 'F' if 0 <= `score` < 80\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pytest unit test function named ``test_grade``\n",
    "\n",
    "# we'll do this as part of the Lab 6 problems..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167b6a9",
   "metadata": {},
   "source": [
    "## HTT-20.8 - Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcacb048",
   "metadata": {},
   "source": [
    "***`assert` statement***\n",
    "\n",
    "- A statement that verifies that a boolean condition is true.\n",
    "\n",
    "***design by contract***\n",
    "\n",
    "- An approach to designing functions that specifies function behavior using preconditions and postconditions.\n",
    "\n",
    "***postcondition***\n",
    "\n",
    "- Part of a function specification that states the work the function completed by the function if the precondition is satisfied.\n",
    "\n",
    "***precondition***\n",
    "\n",
    "- A boolean condition that the caller of a function must ensure is true in order for the function to produce correct results\n",
    "\n",
    "***specification-based tests***\n",
    "\n",
    "- tests that are designed based only on the information in the function specification, without considering any of the details in the function implementation.\n",
    "\n",
    "***unit test***\n",
    "\n",
    "- Code that tests a function to determine if it works properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61089278",
   "metadata": {},
   "source": [
    "### A Short Video Tutorial..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3909d1c",
   "metadata": {},
   "source": [
    "We'll next watch a short course on pytest:\n",
    "\n",
    "https://realpython.com/courses/testing-your-code-with-pytest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f731d0",
   "metadata": {},
   "source": [
    "## Another PyTest Tutorial - with more features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddcd27",
   "metadata": {},
   "source": [
    "based on this site's tutorial:\n",
    "\n",
    "https://realpython.com/pytest-python-testing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86beb2e3",
   "metadata": {},
   "source": [
    "### What Makes `pytest` So Useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf9b78",
   "metadata": {},
   "source": [
    "If you’ve written unit tests for your Python code before, then you may have used Python’s built-in `unittest` module. `unittest` provides a solid base on which to build your test suite, but it has a few shortcomings.\n",
    "\n",
    "A number of third-party testing frameworks attempt to address some of the issues with unittest, and pytest has proven to be one of the most popular. `pytest` is a feature-rich, plugin-based ecosystem for testing your Python code.\n",
    "\n",
    "If you haven’t had the pleasure of using `pytest` yet, then you’re in for a treat! Its philosophy and features will make your testing experience more productive and enjoyable. With pytest, common tasks require less code and advanced tasks can be achieved through a variety of time-saving commands and plugins. It’ll even run your existing tests out of the box, including those written with unittest.\n",
    "\n",
    "As with most frameworks, some development patterns that make sense when you first start using pytest can start causing pains as your test suite grows. This tutorial will help you understand some of the tools pytest provides to keep your testing efficient and effective even as it scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a3adc",
   "metadata": {},
   "source": [
    "### Less Boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8e50a",
   "metadata": {},
   "source": [
    "Most functional tests follow the **Arrange-Act-Assert** model:\n",
    "\n",
    "1. **Arrange**, or set up, the conditions for the test\n",
    "2. **Act** by calling some function or method\n",
    "3. **Assert** that some end condition is true\n",
    "\n",
    "Testing frameworks typically hook into your test’s assertions so that they can provide information when an assertion fails. `unittest`, for example, provides a number of helpful assertion utilities out of the box. However, even a small set of tests requires a fair amount of boilerplate code.\n",
    "\n",
    "Imagine you’d like to write a test suite just to make sure that `unittest` is working properly in your project. You might want to write one test that always passes and one that always fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_unittest.py\n",
    "\n",
    "from unittest import TestCase\n",
    "\n",
    "class TryTesting(TestCase):\n",
    "    def test_always_passes(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "    def test_always_fails(self):\n",
    "        self.assertTrue(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa4025",
   "metadata": {},
   "source": [
    "You can then run those tests from the command line using the discover option of `unittest`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b458928",
   "metadata": {},
   "source": [
    "From the `Terminal` command line, enter:  ```(base) $ python -m unittest discover```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663c1a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python -m unittest discover\n",
    "# ! python -m unittest -q discover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b053f8",
   "metadata": {},
   "source": [
    "As expected, one test passed and one failed. You’ve proven that `unittest` is working, but look at what you had to do:\n",
    "\n",
    "- Import the `TestCase` class from `unittest`\n",
    "- Create `TryTesting`, a subclass of `TestCase`\n",
    "- Write a method in `TryTesting` for each test\n",
    "- Use one of the `self.assert*` methods from `unittest.TestCase` to make assertions\n",
    "\n",
    "That’s a significant amount of code to write, and because it’s the minimum you need for any test, you’d end up writing the same code over and over. `pytest` simplifies this workflow by allowing you to use normal functions and Python’s `assert` keyword directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_pytest.py\n",
    "\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "\n",
    "def test_always_fails():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832957e",
   "metadata": {},
   "source": [
    "That’s it. You don’t have to deal with any imports or classes. All you need to do is include a function with the `test_` prefix. Because you can use the `assert` keyword, you don’t need to learn or remember all the different `self.assert*` methods in `unittest`, either. If you can write an expression that you expect to evaluate to `True`, and then `pytest` will test it for you.\n",
    "\n",
    "Not only does `pytest` eliminate a lot of boilerplate, but it also provides you with a much more detailed and easy-to-read output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa14a71",
   "metadata": {},
   "source": [
    "### Nicer Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad3bf2",
   "metadata": {},
   "source": [
    "You can run your test suite using the pytest command from the top-level folder of your project:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34624b4d",
   "metadata": {},
   "source": [
    "(**not** our Anaconda environment: `venv` != `base`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89688148",
   "metadata": {},
   "source": [
    "(venv) $ pytest\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.10.5, pytest-7.1.2, pluggy-1.0.0\n",
    "rootdir: ...\\effective-python-testing-with-pytest\n",
    "collected 4 items\n",
    "\n",
    "test_with_pytest.py .F                                                   [ 50%]\n",
    "test_with_unittest.py F.                                                 [100%]\n",
    "\n",
    "================================== FAILURES ===================================\n",
    "______________________________ test_always_fails ______________________________\n",
    "\n",
    "    def test_always_fails():\n",
    ">       assert False\n",
    "E       assert False\n",
    "\n",
    "test_with_pytest.py:7: AssertionError\n",
    "________________________ TryTesting.test_always_fails _________________________\n",
    "\n",
    "self = <test_with_unittest.TryTesting testMethod=test_always_fails>\n",
    "\n",
    "    def test_always_fails(self):\n",
    ">       self.assertTrue(False)\n",
    "E       AssertionError: False is not true\n",
    "\n",
    "test_with_unittest.py:10: AssertionError\n",
    "=========================== short test summary info ===========================\n",
    "FAILED test_with_pytest.py::test_always_fails - assert False\n",
    "FAILED test_with_unittest.py::TryTesting::test_always_fails - AssertionError:...\n",
    "\n",
    "========================= 2 failed, 2 passed in 0.20s ========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962a870",
   "metadata": {},
   "source": [
    "`pytest` presents the test results differently than `unittest`, and the `test_with_unittest.py` file was also automatically included. The report shows:\n",
    "\n",
    "1. The system state, including which versions of Python, pytest, and any plugins you have installed\n",
    "2. The rootdir, or the directory to search under for configuration and tests\n",
    "3. The number of tests the runner discovered\n",
    "\n",
    "These items are presented in the first section of the output:\n",
    "\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.10.5, pytest-7.1.2, pluggy-1.0.0\n",
    "rootdir: ...\\effective-python-testing-with-pytest\n",
    "collected 4 items\n",
    "\n",
    "The output then indicates the status of each test using a syntax similar to unittest:\n",
    "\n",
    "    A dot (.) means that the test passed.\n",
    "    An F means that the test has failed.\n",
    "    An E means that the test raised an unexpected exception.\n",
    "\n",
    "The special characters are shown next to the name with the overall progress of the test suite shown on the right:\n",
    "\n",
    "test_with_pytest.py .F                                                   [ 50%]\n",
    "test_with_unittest.py F.                                                 [100%]\n",
    "\n",
    "For tests that fail, the report gives a detailed breakdown of the failure. In the example, the tests failed because assert False always fails:\n",
    "\n",
    "================================== FAILURES ===================================\n",
    "______________________________ test_always_fails ______________________________\n",
    "\n",
    "    def test_always_fails():\n",
    ">       assert False\n",
    "E       assert False\n",
    "\n",
    "test_with_pytest.py:7: AssertionError\n",
    "________________________ TryTesting.test_always_fails _________________________\n",
    "\n",
    "self = <test_with_unittest.TryTesting testMethod=test_always_fails>\n",
    "\n",
    "    def test_always_fails(self):\n",
    ">       self.assertTrue(False)\n",
    "E       AssertionError: False is not true\n",
    "\n",
    "test_with_unittest.py:10: AssertionError\n",
    "\n",
    "This extra output can come in extremely handy when debugging. Finally, the report gives an overall status report of the test suite:\n",
    "\n",
    "=========================== short test summary info ===========================\n",
    "FAILED test_with_pytest.py::test_always_fails - assert False\n",
    "FAILED test_with_unittest.py::TryTesting::test_always_fails - AssertionError:...\n",
    "\n",
    "========================= 2 failed, 2 passed in 0.20s ========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7daf45",
   "metadata": {},
   "source": [
    "When compared to `unittest`, the pytest output is much more informative and readable.\n",
    "\n",
    "In the next section, you’ll take a closer look at how pytest takes advantage of the existing assert keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6945d",
   "metadata": {},
   "source": [
    "### Less to Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62bf654",
   "metadata": {},
   "source": [
    "Being able to use the `assert` keyword is also powerful. If you’ve used it before, then there’s nothing new to learn. Here are a few assertion examples so you can get an idea of the types of test you can make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_assert_examples.py\n",
    "\n",
    "def test_uppercase():\n",
    "    assert \"loud noises\".upper() == \"LOUD NOISES\"\n",
    "\n",
    "def test_reversed():\n",
    "    assert list(reversed([1, 2, 3, 4])) == [4, 3, 2, 1]\n",
    "\n",
    "def test_some_primes():\n",
    "    assert 37 in {\n",
    "        num\n",
    "        for num in range(2, 50)\n",
    "        if not any(num % div == 0 for div in range(2, num))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441511e5",
   "metadata": {},
   "source": [
    "Run them as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f8ac8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest test_assert_examples.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d654f1",
   "metadata": {},
   "source": [
    "They look very much like normal Python functions. All of this makes the learning curve for `pytest` shallower than it is for unittest because you don’t need to learn new constructs to get started.\n",
    "\n",
    "Note that each test is quite small and self-contained. This is common—you’ll see long function names and not a lot going on within a function. This serves mainly to keep your tests isolated from each other, so if something breaks, you know exactly where the problem is. A nice side effect is that the labeling is much better in the output.\n",
    "\n",
    "To see an example of a project that creates a test suite along with the main project, check out the *Build a Hash Table in Python With TDD tutorial* . Additionally, you can work on Python practice problems to try test-driven development yourself while you get ready for your next interview or parse CSV files.\n",
    "\n",
    "In the next section, you’re going to be examining **fixtures**, a great `pytest` feature to help you manage test input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a5a96",
   "metadata": {},
   "source": [
    "### Easier to Manage State and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303fb95",
   "metadata": {},
   "source": [
    "Your tests will often depend on types of data or test doubles that mock objects your code is likely to encounter, such as dictionaries or JSON files.\n",
    "\n",
    "With `unittest`, you might extract these dependencies into `.setUp()` and `.tearDown()` methods so that each test in the class can make use of them. Using these special methods is fine, but as your test classes get larger, you may inadvertently make the test’s dependence entirely implicit. In other words, by looking at one of the many tests in isolation, you may not immediately see that it depends on something else.\n",
    "\n",
    "Over time, implicit dependencies can lead to a complex tangle of code that you have to unwind to make sense of your tests. Tests should help to make your code more understandable. If the tests themselves are difficult to understand, then you may be in trouble!\n",
    "\n",
    "`pytest` takes a different approach. It leads you toward explicit dependency declarations that are still reusable thanks to the availability of ***fixtures***. pytest fixtures are functions that can create data, test doubles, or initialize system state for the test suite. Any test that wants to use a fixture must explicitly use this fixture function as an argument to the test function, so dependencies are always stated up front:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixture_demo.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def example_fixture():\n",
    "    return 1\n",
    "\n",
    "def test_with_fixture(example_fixture):\n",
    "    assert example_fixture == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pytest within the shell (works since above file is in this notebook's folder)\n",
    "\n",
    "! pytest fixture_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc643c3",
   "metadata": {},
   "source": [
    "Looking at the test function, you can immediately tell that it depends on a fixture, without needing to check the whole file for fixture definitions.\n",
    "\n",
    "Note: You usually want to put your tests into their own folder called `tests` at the root level of your project.\n",
    "\n",
    "For more information about structuring a Python application, check out the video course on that very topic.\n",
    "\n",
    "Fixtures can also make use of other fixtures, again by declaring them explicitly as dependencies. That means that, over time, your fixtures can become bulky and modular. Although the ability to insert fixtures into other fixtures provides enormous flexibility, it can also make managing dependencies more challenging as your test suite grows.\n",
    "\n",
    "Later in this tutorial, you’ll learn more about fixtures and try a few techniques for handling these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac8464",
   "metadata": {},
   "source": [
    "### Easy to Filter Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74269c7",
   "metadata": {},
   "source": [
    "As your test suite grows, you may find that you want to run just a few tests on a feature and save the full suite for later. `pytest` provides a few ways of doing this:\n",
    "\n",
    "- *Name-based filtering*: You can limit `pytest` to running only those tests whose fully qualified names match a particular expression. You can do this with the -k parameter.\n",
    "- *Directory scoping*: By default, `pytest` will run only those tests that are in or under the current directory.\n",
    "- *Test categorization*: `pytest` can include or exclude tests from particular categories that you define. You can do this with the `-m` parameter.\n",
    "\n",
    "**Test categorization** in particular is a subtly powerful tool. `pytest` enables you to create **`marks`**, or custom labels, for any test you like. A test may have multiple labels, and you can use them for granular control over which tests to run. Later in this tutorial, you’ll see an example of how `pytest` marks work and learn how to make use of them in a large test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b51a5",
   "metadata": {},
   "source": [
    "### Allows Test Parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbd2db",
   "metadata": {},
   "source": [
    "When you’re testing functions that process data or perform generic transformations, you’ll find yourself writing many similar tests. They may differ only in the input or output of the code being tested. This requires duplicating test code, and doing so can sometimes obscure the behavior that you’re trying to test.\n",
    "\n",
    "`unittest` offers a way of collecting several tests into one, but they don’t show up as individual tests in result reports. If one test fails and the rest pass, then the entire group will still return a single failing result. pytest offers its own solution in which each test can pass or fail independently. You’ll see how to parametrize tests with `pytest` later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c090594",
   "metadata": {},
   "source": [
    "### Has a Plugin-Based Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e988ccb2",
   "metadata": {},
   "source": [
    "One of the most beautiful features of `pytest` is its openness to customization and new features. Almost every piece of the program can be cracked open and changed. As a result, pytest users have developed a rich ecosystem of helpful plugins.\n",
    "\n",
    "Although some pytest plugins focus on specific frameworks like Django, others are applicable to most test suites. You’ll see details on some specific plugins later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90202e3c",
   "metadata": {},
   "source": [
    "### The rest of the tutorial is here:  \n",
    "    \n",
    "https://realpython.com/pytest-python-testing/#fixtures-managing-state-and-dependencies    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
